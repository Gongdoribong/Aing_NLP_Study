{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9e3661-1517-40cd-912f-00882a0ce923",
   "metadata": {},
   "source": [
    "# part 2 word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4718a7cd-7b94-4ec3-b590-fe59b86cd7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기\n",
    "# part 1과 달리 unlabeled 훈련 데이터를 사용\n",
    "\n",
    "train = pd.read_csv(\"C:/Users/hyun/Downloads/labeledTrainData.tsv/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"C:/Users/hyun/Downloads/testData.tsv/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"C:/Users/hyun/Downloads/unlabeledTrainData.tsv/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# 데이터 다 잘 읽었나 확인 (총 100000개)\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" %(train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bdc38d1-eb89-4336-ba1f-f87aa88567b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # 문서를 단어 시퀀스로 바꾸는 함수\n",
    "    # stop words 삭제\n",
    "    # 리스트 형태로 반환\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e753506-d5c4-4346-b0ec-2008a60988d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "nltk.download()\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # sentence 쪼개기\n",
    "    # sentece(=word 리스트) 리스트로 반환\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63072d04-dc9e-4301-8b8f-c0a6aedcecef",
   "metadata": {},
   "source": [
    "### data word2vec 할 준비 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88d8c861-bbf5-4bac-9d2d-5efdbc7cdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyun\\AppData\\Local\\Temp\\ipykernel_28880\\290266847.py:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review).get_text()\n",
      "C:\\Users\\hyun\\AppData\\Local\\Temp\\ipykernel_28880\\290266847.py:12: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review).get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "sentences = [] # sentence 리스트 초기화\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "731aab7e-453e-4b0f-8554-a8d1704acbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796172\n",
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n",
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "# part1과 다른 output 확인\n",
    "\n",
    "print(len(sentences))\n",
    "\n",
    "print(sentences[0])\n",
    "\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe44456a-0584-429c-a2a9-aca989e5bcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 01:11:47,217 : INFO : collecting all words and their counts\n",
      "2024-03-05 01:11:47,219 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-03-05 01:11:47,325 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 01:11:47,448 : INFO : PROGRESS: at sentence #20000, processed 451738 words, keeping 24945 word types\n",
      "2024-03-05 01:11:47,560 : INFO : PROGRESS: at sentence #30000, processed 670859 words, keeping 30027 word types\n",
      "2024-03-05 01:11:47,673 : INFO : PROGRESS: at sentence #40000, processed 896841 words, keeping 34335 word types\n",
      "2024-03-05 01:11:47,776 : INFO : PROGRESS: at sentence #50000, processed 1116082 words, keeping 37751 word types\n",
      "2024-03-05 01:11:47,878 : INFO : PROGRESS: at sentence #60000, processed 1337544 words, keeping 40711 word types\n",
      "2024-03-05 01:11:47,986 : INFO : PROGRESS: at sentence #70000, processed 1560307 words, keeping 43311 word types\n",
      "2024-03-05 01:11:48,090 : INFO : PROGRESS: at sentence #80000, processed 1779516 words, keeping 45707 word types\n",
      "2024-03-05 01:11:48,197 : INFO : PROGRESS: at sentence #90000, processed 2003714 words, keeping 48121 word types\n",
      "2024-03-05 01:11:48,304 : INFO : PROGRESS: at sentence #100000, processed 2225465 words, keeping 50190 word types\n",
      "2024-03-05 01:11:48,413 : INFO : PROGRESS: at sentence #110000, processed 2444323 words, keeping 52058 word types\n",
      "2024-03-05 01:11:48,527 : INFO : PROGRESS: at sentence #120000, processed 2666488 words, keeping 54098 word types\n",
      "2024-03-05 01:11:48,641 : INFO : PROGRESS: at sentence #130000, processed 2892315 words, keeping 55837 word types\n",
      "2024-03-05 01:11:48,744 : INFO : PROGRESS: at sentence #140000, processed 3104796 words, keeping 57324 word types\n",
      "2024-03-05 01:11:48,854 : INFO : PROGRESS: at sentence #150000, processed 3330432 words, keeping 59045 word types\n",
      "2024-03-05 01:11:48,962 : INFO : PROGRESS: at sentence #160000, processed 3552466 words, keeping 60581 word types\n",
      "2024-03-05 01:11:49,067 : INFO : PROGRESS: at sentence #170000, processed 3776048 words, keeping 62050 word types\n",
      "2024-03-05 01:11:49,178 : INFO : PROGRESS: at sentence #180000, processed 3996237 words, keeping 63483 word types\n",
      "2024-03-05 01:11:49,281 : INFO : PROGRESS: at sentence #190000, processed 4221288 words, keeping 64775 word types\n",
      "2024-03-05 01:11:49,387 : INFO : PROGRESS: at sentence #200000, processed 4445973 words, keeping 66070 word types\n",
      "2024-03-05 01:11:49,492 : INFO : PROGRESS: at sentence #210000, processed 4666511 words, keeping 67367 word types\n",
      "2024-03-05 01:11:49,599 : INFO : PROGRESS: at sentence #220000, processed 4892037 words, keeping 68686 word types\n",
      "2024-03-05 01:11:49,706 : INFO : PROGRESS: at sentence #230000, processed 5113881 words, keeping 69935 word types\n",
      "2024-03-05 01:11:49,818 : INFO : PROGRESS: at sentence #240000, processed 5340847 words, keeping 71144 word types\n",
      "2024-03-05 01:11:49,922 : INFO : PROGRESS: at sentence #250000, processed 5555463 words, keeping 72333 word types\n",
      "2024-03-05 01:11:50,025 : INFO : PROGRESS: at sentence #260000, processed 5775304 words, keeping 73466 word types\n",
      "2024-03-05 01:11:50,130 : INFO : PROGRESS: at sentence #270000, processed 5995572 words, keeping 74740 word types\n",
      "2024-03-05 01:11:50,244 : INFO : PROGRESS: at sentence #280000, processed 6220911 words, keeping 76318 word types\n",
      "2024-03-05 01:11:50,348 : INFO : PROGRESS: at sentence #290000, processed 6443523 words, keeping 77787 word types\n",
      "2024-03-05 01:11:50,456 : INFO : PROGRESS: at sentence #300000, processed 6668258 words, keeping 79142 word types\n",
      "2024-03-05 01:11:50,564 : INFO : PROGRESS: at sentence #310000, processed 6892662 words, keeping 80431 word types\n",
      "2024-03-05 01:11:50,669 : INFO : PROGRESS: at sentence #320000, processed 7118969 words, keeping 81794 word types\n",
      "2024-03-05 01:11:50,774 : INFO : PROGRESS: at sentence #330000, processed 7340486 words, keeping 83006 word types\n",
      "2024-03-05 01:11:50,881 : INFO : PROGRESS: at sentence #340000, processed 7569986 words, keeping 84252 word types\n",
      "2024-03-05 01:11:50,986 : INFO : PROGRESS: at sentence #350000, processed 7792927 words, keeping 85407 word types\n",
      "2024-03-05 01:11:51,086 : INFO : PROGRESS: at sentence #360000, processed 8012526 words, keeping 86567 word types\n",
      "2024-03-05 01:11:51,204 : INFO : PROGRESS: at sentence #370000, processed 8239772 words, keeping 87663 word types\n",
      "2024-03-05 01:11:51,316 : INFO : PROGRESS: at sentence #380000, processed 8465827 words, keeping 88849 word types\n",
      "2024-03-05 01:11:51,438 : INFO : PROGRESS: at sentence #390000, processed 8694607 words, keeping 89883 word types\n",
      "2024-03-05 01:11:51,545 : INFO : PROGRESS: at sentence #400000, processed 8917820 words, keeping 90882 word types\n",
      "2024-03-05 01:11:51,649 : INFO : PROGRESS: at sentence #410000, processed 9138504 words, keeping 91859 word types\n",
      "2024-03-05 01:11:51,754 : INFO : PROGRESS: at sentence #420000, processed 9358474 words, keeping 92880 word types\n",
      "2024-03-05 01:11:51,858 : INFO : PROGRESS: at sentence #430000, processed 9586958 words, keeping 93909 word types\n",
      "2024-03-05 01:11:51,968 : INFO : PROGRESS: at sentence #440000, processed 9812576 words, keeping 94853 word types\n",
      "2024-03-05 01:11:52,074 : INFO : PROGRESS: at sentence #450000, processed 10036719 words, keeping 95995 word types\n",
      "2024-03-05 01:11:52,191 : INFO : PROGRESS: at sentence #460000, processed 10269931 words, keeping 97064 word types\n",
      "2024-03-05 01:11:52,299 : INFO : PROGRESS: at sentence #470000, processed 10496262 words, keeping 97885 word types\n",
      "2024-03-05 01:11:52,405 : INFO : PROGRESS: at sentence #480000, processed 10717170 words, keeping 98809 word types\n",
      "2024-03-05 01:11:52,510 : INFO : PROGRESS: at sentence #490000, processed 10943335 words, keeping 99835 word types\n",
      "2024-03-05 01:11:52,618 : INFO : PROGRESS: at sentence #500000, processed 11165141 words, keeping 100726 word types\n",
      "2024-03-05 01:11:52,721 : INFO : PROGRESS: at sentence #510000, processed 11390498 words, keeping 101672 word types\n",
      "2024-03-05 01:11:52,827 : INFO : PROGRESS: at sentence #520000, processed 11613511 words, keeping 102557 word types\n",
      "2024-03-05 01:11:52,938 : INFO : PROGRESS: at sentence #530000, processed 11838774 words, keeping 103374 word types\n",
      "2024-03-05 01:11:53,048 : INFO : PROGRESS: at sentence #540000, processed 12062185 words, keeping 104231 word types\n",
      "2024-03-05 01:11:53,162 : INFO : PROGRESS: at sentence #550000, processed 12286959 words, keeping 105098 word types\n",
      "2024-03-05 01:11:53,267 : INFO : PROGRESS: at sentence #560000, processed 12509034 words, keeping 105971 word types\n",
      "2024-03-05 01:11:53,379 : INFO : PROGRESS: at sentence #570000, processed 12736827 words, keeping 106757 word types\n",
      "2024-03-05 01:11:53,477 : INFO : PROGRESS: at sentence #580000, processed 12958427 words, keeping 107611 word types\n",
      "2024-03-05 01:11:53,578 : INFO : PROGRESS: at sentence #590000, processed 13184325 words, keeping 108468 word types\n",
      "2024-03-05 01:11:53,687 : INFO : PROGRESS: at sentence #600000, processed 13406551 words, keeping 109189 word types\n",
      "2024-03-05 01:11:53,793 : INFO : PROGRESS: at sentence #610000, processed 13628198 words, keeping 110055 word types\n",
      "2024-03-05 01:11:53,899 : INFO : PROGRESS: at sentence #620000, processed 13852588 words, keeping 110805 word types\n",
      "2024-03-05 01:11:54,005 : INFO : PROGRESS: at sentence #630000, processed 14075901 words, keeping 111573 word types\n",
      "2024-03-05 01:11:54,112 : INFO : PROGRESS: at sentence #640000, processed 14298046 words, keeping 112386 word types\n",
      "2024-03-05 01:11:54,229 : INFO : PROGRESS: at sentence #650000, processed 14522874 words, keeping 113151 word types\n",
      "2024-03-05 01:11:54,337 : INFO : PROGRESS: at sentence #660000, processed 14745445 words, keeping 113890 word types\n",
      "2024-03-05 01:11:54,441 : INFO : PROGRESS: at sentence #670000, processed 14970569 words, keeping 114613 word types\n",
      "2024-03-05 01:11:54,546 : INFO : PROGRESS: at sentence #680000, processed 15194625 words, keeping 115331 word types\n",
      "2024-03-05 01:11:54,652 : INFO : PROGRESS: at sentence #690000, processed 15416773 words, keeping 116099 word types\n",
      "2024-03-05 01:11:54,761 : INFO : PROGRESS: at sentence #700000, processed 15645695 words, keeping 116902 word types\n",
      "2024-03-05 01:11:54,864 : INFO : PROGRESS: at sentence #710000, processed 15865815 words, keeping 117541 word types\n",
      "2024-03-05 01:11:54,972 : INFO : PROGRESS: at sentence #720000, processed 16093342 words, keeping 118183 word types\n",
      "2024-03-05 01:11:55,089 : INFO : PROGRESS: at sentence #730000, processed 16316787 words, keeping 118912 word types\n",
      "2024-03-05 01:11:55,199 : INFO : PROGRESS: at sentence #740000, processed 16539147 words, keeping 119618 word types\n",
      "2024-03-05 01:11:55,301 : INFO : PROGRESS: at sentence #750000, processed 16758552 words, keeping 120264 word types\n",
      "2024-03-05 01:11:55,406 : INFO : PROGRESS: at sentence #760000, processed 16977111 words, keeping 120888 word types\n",
      "2024-03-05 01:11:55,512 : INFO : PROGRESS: at sentence #770000, processed 17203259 words, keeping 121656 word types\n",
      "2024-03-05 01:11:55,622 : INFO : PROGRESS: at sentence #780000, processed 17432844 words, keeping 122358 word types\n",
      "2024-03-05 01:11:55,730 : INFO : PROGRESS: at sentence #790000, processed 17660151 words, keeping 123033 word types\n",
      "2024-03-05 01:11:55,798 : INFO : collected 123504 word types from a corpus of 17798270 raw words and 796172 sentences\n",
      "2024-03-05 01:11:55,799 : INFO : Creating a fresh vocabulary\n",
      "2024-03-05 01:11:56,031 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16490 unique words (13.35% of original 123504, drops 107014)', 'datetime': '2024-03-05T01:11:56.031547', 'gensim': '4.3.2', 'python': '3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2024-03-05 01:11:56,033 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 17239125 word corpus (96.86% of original 17798270, drops 559145)', 'datetime': '2024-03-05T01:11:56.033530', 'gensim': '4.3.2', 'python': '3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2024-03-05 01:11:56,248 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2024-03-05 01:11:56,253 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2024-03-05 01:11:56,255 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12749798.434354488 word corpus (74.0%% of prior 17239125)', 'datetime': '2024-03-05T01:11:56.255019', 'gensim': '4.3.2', 'python': '3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2024-03-05 01:11:56,585 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2024-03-05 01:11:56,586 : INFO : resetting layer weights\n",
      "2024-03-05 01:11:56,632 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-03-05T01:11:56.632194', 'gensim': '4.3.2', 'python': '3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "2024-03-05 01:11:56,634 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-03-05T01:11:56.634123', 'gensim': '4.3.2', 'python': '3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2024-03-05 01:11:57,651 : INFO : EPOCH 0 - PROGRESS: at 3.59% examples, 456761 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:11:58,669 : INFO : EPOCH 0 - PROGRESS: at 7.36% examples, 463280 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:11:59,681 : INFO : EPOCH 0 - PROGRESS: at 11.14% examples, 466699 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:00,688 : INFO : EPOCH 0 - PROGRESS: at 14.83% examples, 465610 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:12:01,712 : INFO : EPOCH 0 - PROGRESS: at 18.69% examples, 467357 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:02,714 : INFO : EPOCH 0 - PROGRESS: at 22.48% examples, 469211 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:03,745 : INFO : EPOCH 0 - PROGRESS: at 26.40% examples, 471585 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:04,754 : INFO : EPOCH 0 - PROGRESS: at 30.26% examples, 473622 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:05,771 : INFO : EPOCH 0 - PROGRESS: at 34.22% examples, 474931 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:06,793 : INFO : EPOCH 0 - PROGRESS: at 38.01% examples, 474990 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:07,808 : INFO : EPOCH 0 - PROGRESS: at 41.81% examples, 475350 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:08,809 : INFO : EPOCH 0 - PROGRESS: at 45.55% examples, 475604 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:09,819 : INFO : EPOCH 0 - PROGRESS: at 49.15% examples, 474466 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:12:10,835 : INFO : EPOCH 0 - PROGRESS: at 52.94% examples, 474223 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:11,857 : INFO : EPOCH 0 - PROGRESS: at 56.71% examples, 474268 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:12,863 : INFO : EPOCH 0 - PROGRESS: at 60.47% examples, 474813 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:13,867 : INFO : EPOCH 0 - PROGRESS: at 64.28% examples, 475419 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:14,878 : INFO : EPOCH 0 - PROGRESS: at 67.98% examples, 474912 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:12:15,884 : INFO : EPOCH 0 - PROGRESS: at 71.71% examples, 475011 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:16,891 : INFO : EPOCH 0 - PROGRESS: at 75.48% examples, 475064 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:17,914 : INFO : EPOCH 0 - PROGRESS: at 79.23% examples, 474739 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:18,917 : INFO : EPOCH 0 - PROGRESS: at 82.94% examples, 474531 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:19,918 : INFO : EPOCH 0 - PROGRESS: at 86.70% examples, 474702 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:12:20,929 : INFO : EPOCH 0 - PROGRESS: at 90.42% examples, 474662 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:21,947 : INFO : EPOCH 0 - PROGRESS: at 94.21% examples, 474494 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:22,949 : INFO : EPOCH 0 - PROGRESS: at 97.95% examples, 474648 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:12:23,476 : INFO : EPOCH 0: training on 17798270 raw words (12747874 effective words) took 26.8s, 475159 effective words/s\n",
      "2024-03-05 01:12:24,500 : INFO : EPOCH 1 - PROGRESS: at 3.59% examples, 455085 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:25,508 : INFO : EPOCH 1 - PROGRESS: at 7.24% examples, 457795 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:26,530 : INFO : EPOCH 1 - PROGRESS: at 11.03% examples, 461614 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:27,545 : INFO : EPOCH 1 - PROGRESS: at 14.89% examples, 466159 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:28,548 : INFO : EPOCH 1 - PROGRESS: at 18.64% examples, 466957 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:29,548 : INFO : EPOCH 1 - PROGRESS: at 22.26% examples, 465416 words/s, in_qsize 6, out_qsize 1\n",
      "2024-03-05 01:12:30,568 : INFO : EPOCH 1 - PROGRESS: at 25.96% examples, 465036 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:31,580 : INFO : EPOCH 1 - PROGRESS: at 29.76% examples, 466900 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:32,585 : INFO : EPOCH 1 - PROGRESS: at 33.66% examples, 468639 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:33,590 : INFO : EPOCH 1 - PROGRESS: at 37.51% examples, 470862 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:12:34,603 : INFO : EPOCH 1 - PROGRESS: at 41.27% examples, 471048 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:35,625 : INFO : EPOCH 1 - PROGRESS: at 45.06% examples, 471473 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:36,635 : INFO : EPOCH 1 - PROGRESS: at 48.77% examples, 471705 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:37,636 : INFO : EPOCH 1 - PROGRESS: at 52.43% examples, 471198 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:12:38,649 : INFO : EPOCH 1 - PROGRESS: at 56.00% examples, 469871 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:39,650 : INFO : EPOCH 1 - PROGRESS: at 59.73% examples, 470838 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:40,680 : INFO : EPOCH 1 - PROGRESS: at 63.51% examples, 470496 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:41,711 : INFO : EPOCH 1 - PROGRESS: at 67.01% examples, 468615 words/s, in_qsize 8, out_qsize 1\n",
      "2024-03-05 01:12:42,713 : INFO : EPOCH 1 - PROGRESS: at 70.88% examples, 469882 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:43,724 : INFO : EPOCH 1 - PROGRESS: at 74.57% examples, 469748 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:44,726 : INFO : EPOCH 1 - PROGRESS: at 78.27% examples, 469812 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:45,732 : INFO : EPOCH 1 - PROGRESS: at 82.04% examples, 470137 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:46,751 : INFO : EPOCH 1 - PROGRESS: at 85.74% examples, 469838 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:47,759 : INFO : EPOCH 1 - PROGRESS: at 89.53% examples, 470337 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:48,762 : INFO : EPOCH 1 - PROGRESS: at 93.23% examples, 470369 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:49,764 : INFO : EPOCH 1 - PROGRESS: at 96.96% examples, 470374 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:50,563 : INFO : EPOCH 1: training on 17798270 raw words (12749598 effective words) took 27.1s, 470999 effective words/s\n",
      "2024-03-05 01:12:51,581 : INFO : EPOCH 2 - PROGRESS: at 3.59% examples, 454761 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:52,587 : INFO : EPOCH 2 - PROGRESS: at 7.30% examples, 461574 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:53,608 : INFO : EPOCH 2 - PROGRESS: at 11.03% examples, 461801 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:54,610 : INFO : EPOCH 2 - PROGRESS: at 14.72% examples, 462537 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:55,621 : INFO : EPOCH 2 - PROGRESS: at 18.47% examples, 463364 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:56,625 : INFO : EPOCH 2 - PROGRESS: at 22.20% examples, 464469 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:57,629 : INFO : EPOCH 2 - PROGRESS: at 26.07% examples, 468283 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:58,631 : INFO : EPOCH 2 - PROGRESS: at 29.76% examples, 468547 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:12:59,632 : INFO : EPOCH 2 - PROGRESS: at 33.61% examples, 469566 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:00,637 : INFO : EPOCH 2 - PROGRESS: at 37.22% examples, 468875 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:01,653 : INFO : EPOCH 2 - PROGRESS: at 41.03% examples, 469744 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:02,666 : INFO : EPOCH 2 - PROGRESS: at 44.79% examples, 470109 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:03,668 : INFO : EPOCH 2 - PROGRESS: at 48.61% examples, 471801 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:04,683 : INFO : EPOCH 2 - PROGRESS: at 52.37% examples, 471793 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:05,691 : INFO : EPOCH 2 - PROGRESS: at 56.06% examples, 471528 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:13:06,706 : INFO : EPOCH 2 - PROGRESS: at 59.73% examples, 471536 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:07,727 : INFO : EPOCH 2 - PROGRESS: at 63.55% examples, 471764 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:08,730 : INFO : EPOCH 2 - PROGRESS: at 67.24% examples, 471719 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:09,745 : INFO : EPOCH 2 - PROGRESS: at 70.83% examples, 470631 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:13:10,747 : INFO : EPOCH 2 - PROGRESS: at 74.51% examples, 470643 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:11,761 : INFO : EPOCH 2 - PROGRESS: at 78.11% examples, 469720 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:13:12,766 : INFO : EPOCH 2 - PROGRESS: at 81.87% examples, 470048 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:13,778 : INFO : EPOCH 2 - PROGRESS: at 85.63% examples, 470199 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:13:14,779 : INFO : EPOCH 2 - PROGRESS: at 89.32% examples, 470258 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:15,791 : INFO : EPOCH 2 - PROGRESS: at 93.07% examples, 470389 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:16,805 : INFO : EPOCH 2 - PROGRESS: at 96.86% examples, 470460 words/s, in_qsize 6, out_qsize 1\n",
      "2024-03-05 01:13:17,648 : INFO : EPOCH 2: training on 17798270 raw words (12747786 effective words) took 27.1s, 470867 effective words/s\n",
      "2024-03-05 01:13:18,670 : INFO : EPOCH 3 - PROGRESS: at 3.54% examples, 446959 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:19,673 : INFO : EPOCH 3 - PROGRESS: at 7.24% examples, 458265 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:20,690 : INFO : EPOCH 3 - PROGRESS: at 11.14% examples, 467454 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:21,700 : INFO : EPOCH 3 - PROGRESS: at 14.83% examples, 465723 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:22,703 : INFO : EPOCH 3 - PROGRESS: at 18.64% examples, 468056 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:23,707 : INFO : EPOCH 3 - PROGRESS: at 22.37% examples, 468494 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:24,708 : INFO : EPOCH 3 - PROGRESS: at 26.01% examples, 467897 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:13:25,738 : INFO : EPOCH 3 - PROGRESS: at 29.82% examples, 468443 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:26,749 : INFO : EPOCH 3 - PROGRESS: at 33.66% examples, 468921 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:27,754 : INFO : EPOCH 3 - PROGRESS: at 37.28% examples, 468296 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:28,758 : INFO : EPOCH 3 - PROGRESS: at 41.03% examples, 469115 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:29,771 : INFO : EPOCH 3 - PROGRESS: at 44.84% examples, 470063 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:30,796 : INFO : EPOCH 3 - PROGRESS: at 48.61% examples, 470346 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:31,800 : INFO : EPOCH 3 - PROGRESS: at 52.37% examples, 470813 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:32,803 : INFO : EPOCH 3 - PROGRESS: at 56.11% examples, 471257 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:33,808 : INFO : EPOCH 3 - PROGRESS: at 59.78% examples, 471578 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:34,819 : INFO : EPOCH 3 - PROGRESS: at 63.61% examples, 472107 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:35,833 : INFO : EPOCH 3 - PROGRESS: at 67.29% examples, 471789 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:36,837 : INFO : EPOCH 3 - PROGRESS: at 71.16% examples, 472855 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:37,867 : INFO : EPOCH 3 - PROGRESS: at 74.86% examples, 472106 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:38,870 : INFO : EPOCH 3 - PROGRESS: at 78.56% examples, 472041 words/s, in_qsize 8, out_qsize 1\n",
      "2024-03-05 01:13:39,880 : INFO : EPOCH 3 - PROGRESS: at 82.38% examples, 472459 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:40,881 : INFO : EPOCH 3 - PROGRESS: at 86.08% examples, 472450 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:41,892 : INFO : EPOCH 3 - PROGRESS: at 89.68% examples, 471904 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:42,902 : INFO : EPOCH 3 - PROGRESS: at 93.46% examples, 471983 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:13:43,929 : INFO : EPOCH 3 - PROGRESS: at 97.18% examples, 471496 words/s, in_qsize 8, out_qsize 1\n",
      "2024-03-05 01:13:44,658 : INFO : EPOCH 3: training on 17798270 raw words (12749997 effective words) took 27.0s, 472290 effective words/s\n",
      "2024-03-05 01:13:45,676 : INFO : EPOCH 4 - PROGRESS: at 3.54% examples, 447728 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:46,685 : INFO : EPOCH 4 - PROGRESS: at 7.19% examples, 453782 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:13:47,691 : INFO : EPOCH 4 - PROGRESS: at 11.03% examples, 463727 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:48,692 : INFO : EPOCH 4 - PROGRESS: at 14.83% examples, 467545 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:49,698 : INFO : EPOCH 4 - PROGRESS: at 18.47% examples, 465014 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:50,708 : INFO : EPOCH 4 - PROGRESS: at 22.08% examples, 462982 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:13:51,726 : INFO : EPOCH 4 - PROGRESS: at 25.85% examples, 464018 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:52,740 : INFO : EPOCH 4 - PROGRESS: at 29.58% examples, 464975 words/s, in_qsize 8, out_qsize 1\n",
      "2024-03-05 01:13:53,761 : INFO : EPOCH 4 - PROGRESS: at 33.50% examples, 466225 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:54,762 : INFO : EPOCH 4 - PROGRESS: at 37.17% examples, 466768 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:55,762 : INFO : EPOCH 4 - PROGRESS: at 40.74% examples, 465942 words/s, in_qsize 6, out_qsize 1\n",
      "2024-03-05 01:13:56,775 : INFO : EPOCH 4 - PROGRESS: at 44.50% examples, 466567 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:13:57,793 : INFO : EPOCH 4 - PROGRESS: at 48.32% examples, 467916 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:58,798 : INFO : EPOCH 4 - PROGRESS: at 52.10% examples, 468565 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:13:59,822 : INFO : EPOCH 4 - PROGRESS: at 55.89% examples, 468973 words/s, in_qsize 8, out_qsize 0\n",
      "2024-03-05 01:14:00,823 : INFO : EPOCH 4 - PROGRESS: at 59.40% examples, 468255 words/s, in_qsize 6, out_qsize 1\n",
      "2024-03-05 01:14:01,842 : INFO : EPOCH 4 - PROGRESS: at 63.16% examples, 468388 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:14:02,859 : INFO : EPOCH 4 - PROGRESS: at 66.90% examples, 468540 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:14:03,861 : INFO : EPOCH 4 - PROGRESS: at 70.72% examples, 469433 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:14:04,864 : INFO : EPOCH 4 - PROGRESS: at 74.40% examples, 469485 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:14:05,889 : INFO : EPOCH 4 - PROGRESS: at 78.16% examples, 469376 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:14:06,895 : INFO : EPOCH 4 - PROGRESS: at 81.87% examples, 469357 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:14:07,915 : INFO : EPOCH 4 - PROGRESS: at 85.63% examples, 469393 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:14:08,923 : INFO : EPOCH 4 - PROGRESS: at 89.26% examples, 469014 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:14:09,936 : INFO : EPOCH 4 - PROGRESS: at 92.85% examples, 468352 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:14:10,961 : INFO : EPOCH 4 - PROGRESS: at 96.64% examples, 468310 words/s, in_qsize 7, out_qsize 0\n",
      "2024-03-05 01:14:11,884 : INFO : EPOCH 4: training on 17798270 raw words (12748394 effective words) took 27.2s, 468443 effective words/s\n",
      "2024-03-05 01:14:11,885 : INFO : Word2Vec lifecycle event {'msg': 'training on 88991350 raw words (63743649 effective words) took 135.3s, 471300 effective words/s', 'datetime': '2024-03-05T01:14:11.885683', 'gensim': '4.3.2', 'python': '3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2024-03-05 01:14:11,887 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16490, vector_size=300, alpha=0.025>', 'datetime': '2024-03-05T01:14:11.887687', 'gensim': '4.3.2', 'python': '3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "C:\\Users\\hyun\\AppData\\Local\\Temp\\ipykernel_28880\\3491330449.py:28: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n",
      "2024-03-05 01:14:11,913 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "2024-03-05 01:14:11,920 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-03-05T01:14:11.920708', 'gensim': '4.3.2', 'python': '3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'saving'}\n",
      "2024-03-05 01:14:11,921 : INFO : not storing attribute cum_table\n",
      "2024-03-05 01:14:12,000 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "####\n",
    "# 위에 파라미터에서 오류!\n",
    "# __init__() got an unexpected keyword argument 'size' 라는 에러가 나왔는데\n",
    "# 버전 문제로, 설명은 옛날 글인데 내가 설치한 버전이 더 최신이라 생긴 문제\n",
    "# size 파라미터를 vector_size로 바꿔주니 해결!\n",
    "####\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1dd4cc-c841-4a10-9449-49080fbf7a17",
   "metadata": {},
   "source": [
    "### 모델링 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bc08db4-9a58-47f1-aa05-8ffaa00af55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뜻이 가장 다른 단어 찾기\n",
    "## 이 또한 버전 업데이트로.. 에러 발생\n",
    "## 'Word2Vec' object has no attribute 'doesnt_match' 라는데\n",
    "## 'doesnt_match' 함수가 Word2Vec 객체의 wv 안으로 들어감(왠만한건 다 wv 붙여야하는듯)\n",
    "#\n",
    "\n",
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29d10b1a-6f37-4065-8f30-041b347850fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21afe3cf-d4d3-421d-9b2f-e62c150f42d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44ca8efd-12ec-40da-b5f4-6200cdcc9aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6181036233901978),\n",
       " ('lady', 0.5941401720046997),\n",
       " ('lad', 0.5537906289100647),\n",
       " ('monk', 0.5482915043830872),\n",
       " ('guy', 0.5263317823410034),\n",
       " ('farmer', 0.5231702923774719),\n",
       " ('men', 0.5203357934951782),\n",
       " ('soldier', 0.5164754390716553),\n",
       " ('businessman', 0.5013821721076965),\n",
       " ('millionaire', 0.501152515411377)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c643109-0399-4ebe-bca5-467aad643fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_book",
   "language": "python",
   "name": "tf2_book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
