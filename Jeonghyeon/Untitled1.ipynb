{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1GhT_5J0moQQxCrNblT6mp5k0l9ObpGyW","authorship_tag":"ABX9TyOIs2BmBM9jg4EF4UAMjxgO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":10,"metadata":{"id":"dU3N0K901yCv","executionInfo":{"status":"ok","timestamp":1709630601956,"user_tz":-540,"elapsed":2202,"user":{"displayName":"현","userId":"10086973512213957086"}}},"outputs":[],"source":["# Import the pandas package, then use the \"read_csv\" function to read\n","# the labeled training data\n","import pandas as pd\n","train = pd.read_csv(\"/content/drive/MyDrive/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"]},{"cell_type":"code","source":[],"metadata":{"id":"gN7u5GYl8amL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.shape\n","(25000, 3)\n","\n","train.columns.values\n","print(train.columns.values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CwkqxafH14rR","executionInfo":{"status":"ok","timestamp":1709630947845,"user_tz":-540,"elapsed":6,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"c6a98625-f256-4896-a3f8-1841655e3b85"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["['id' 'sentiment' 'review']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iRTOzuhQ6CJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import BeautifulSoup into your workspace\n","from bs4 import BeautifulSoup\n","\n","# Initialize the BeautifulSoup object on a single movie review\n","example1 = BeautifulSoup(train[\"review\"][0])\n","\n","# Print the raw review and then the output of get_text(), for\n","# comparison\n","print (train[\"review\"][0])\n","print (example1.get_text())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFrlf77a15qn","executionInfo":{"status":"ok","timestamp":1709630953850,"user_tz":-540,"elapsed":275,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"663d868a-c38f-4de6-8554-930d1e9fc8d5"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n","\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"]}]},{"cell_type":"code","source":["import re\n","# Use regular expressions to do a find-and-replace\n","letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n","                      \" \",                   # The pattern to replace it with\n","                      example1.get_text() )  # The text to search\n","print (letters_only)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xvkYlB9b150O","executionInfo":{"status":"ok","timestamp":1709630969069,"user_tz":-540,"elapsed":251,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"918133e0-45f2-44d6-9dd6-12c6aa6c7ab8"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":[" With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring  Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him The actual feature film bit when it finally starts is only on for    minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord  Why he wants MJ dead so bad is beyond me  Because MJ overheard his plans  Nah  Joe Pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno  maybe he just hates MJ s music Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence  Also  the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene Bottom line  this movie is for people who like MJ on one level or another  which i think is most people   If not  then stay away  It does try and give off a wholesome message and ironically MJ s bestest buddy in this movie is a girl  Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty  Well  with all the attention i ve gave this subject    hmmm well i don t know because people can be different behind closed doors  i know this for a fact  He is either an extremely nice but stupid guy or one of the most sickest liars  I hope he is not the latter  \n"]}]},{"cell_type":"code","source":["lower_case = letters_only.lower()        # Convert to lower case\n","words = lower_case.split()               # Split into words"],"metadata":{"id":"9GCVGgs3153X","executionInfo":{"status":"ok","timestamp":1709630972052,"user_tz":-540,"elapsed":243,"user":{"displayName":"현","userId":"10086973512213957086"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download()  # Download text data sets, including stop words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lv0UXrHL156U","executionInfo":{"status":"ok","timestamp":1709631079784,"user_tz":-540,"elapsed":106150,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"9e26c8aa-36d9-4ac6-f09d-79a3911403e8"},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> d\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> all\n"]},{"output_type":"stream","name":"stderr","text":["    Downloading collection 'all'\n","       | \n","       | Downloading package abc to /root/nltk_data...\n","       |   Unzipping corpora/abc.zip.\n","       | Downloading package alpino to /root/nltk_data...\n","       |   Unzipping corpora/alpino.zip.\n","       | Downloading package averaged_perceptron_tagger to\n","       |     /root/nltk_data...\n","       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","       | Downloading package averaged_perceptron_tagger_ru to\n","       |     /root/nltk_data...\n","       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n","       | Downloading package basque_grammars to /root/nltk_data...\n","       |   Unzipping grammars/basque_grammars.zip.\n","       | Downloading package bcp47 to /root/nltk_data...\n","       | Downloading package biocreative_ppi to /root/nltk_data...\n","       |   Unzipping corpora/biocreative_ppi.zip.\n","       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n","       |   Unzipping models/bllip_wsj_no_aux.zip.\n","       | Downloading package book_grammars to /root/nltk_data...\n","       |   Unzipping grammars/book_grammars.zip.\n","       | Downloading package brown to /root/nltk_data...\n","       |   Unzipping corpora/brown.zip.\n","       | Downloading package brown_tei to /root/nltk_data...\n","       |   Unzipping corpora/brown_tei.zip.\n","       | Downloading package cess_cat to /root/nltk_data...\n","       |   Unzipping corpora/cess_cat.zip.\n","       | Downloading package cess_esp to /root/nltk_data...\n","       |   Unzipping corpora/cess_esp.zip.\n","       | Downloading package chat80 to /root/nltk_data...\n","       |   Unzipping corpora/chat80.zip.\n","       | Downloading package city_database to /root/nltk_data...\n","       |   Unzipping corpora/city_database.zip.\n","       | Downloading package cmudict to /root/nltk_data...\n","       |   Unzipping corpora/cmudict.zip.\n","       | Downloading package comparative_sentences to\n","       |     /root/nltk_data...\n","       |   Unzipping corpora/comparative_sentences.zip.\n","       | Downloading package comtrans to /root/nltk_data...\n","       | Downloading package conll2000 to /root/nltk_data...\n","       |   Unzipping corpora/conll2000.zip.\n","       | Downloading package conll2002 to /root/nltk_data...\n","       |   Unzipping corpora/conll2002.zip.\n","       | Downloading package conll2007 to /root/nltk_data...\n","       | Downloading package crubadan to /root/nltk_data...\n","       |   Unzipping corpora/crubadan.zip.\n","       | Downloading package dependency_treebank to /root/nltk_data...\n","       |   Unzipping corpora/dependency_treebank.zip.\n","       | Downloading package dolch to /root/nltk_data...\n","       |   Unzipping corpora/dolch.zip.\n","       | Downloading package europarl_raw to /root/nltk_data...\n","       |   Unzipping corpora/europarl_raw.zip.\n","       | Downloading package extended_omw to /root/nltk_data...\n","       | Downloading package floresta to /root/nltk_data...\n","       |   Unzipping corpora/floresta.zip.\n","       | Downloading package framenet_v15 to /root/nltk_data...\n","       |   Unzipping corpora/framenet_v15.zip.\n","       | Downloading package framenet_v17 to /root/nltk_data...\n","       |   Unzipping corpora/framenet_v17.zip.\n","       | Downloading package gazetteers to /root/nltk_data...\n","       |   Unzipping corpora/gazetteers.zip.\n","       | Downloading package genesis to /root/nltk_data...\n","       |   Unzipping corpora/genesis.zip.\n","       | Downloading package gutenberg to /root/nltk_data...\n","       |   Unzipping corpora/gutenberg.zip.\n","       | Downloading package ieer to /root/nltk_data...\n","       |   Unzipping corpora/ieer.zip.\n","       | Downloading package inaugural to /root/nltk_data...\n","       |   Unzipping corpora/inaugural.zip.\n","       | Downloading package indian to /root/nltk_data...\n","       |   Unzipping corpora/indian.zip.\n","       | Downloading package jeita to /root/nltk_data...\n","       | Downloading package kimmo to /root/nltk_data...\n","       |   Unzipping corpora/kimmo.zip.\n","       | Downloading package knbc to /root/nltk_data...\n","       | Downloading package large_grammars to /root/nltk_data...\n","       |   Unzipping grammars/large_grammars.zip.\n","       | Downloading package lin_thesaurus to /root/nltk_data...\n","       |   Unzipping corpora/lin_thesaurus.zip.\n","       | Downloading package mac_morpho to /root/nltk_data...\n","       |   Unzipping corpora/mac_morpho.zip.\n","       | Downloading package machado to /root/nltk_data...\n","       | Downloading package masc_tagged to /root/nltk_data...\n","       | Downloading package maxent_ne_chunker to /root/nltk_data...\n","       |   Unzipping chunkers/maxent_ne_chunker.zip.\n","       | Downloading package maxent_treebank_pos_tagger to\n","       |     /root/nltk_data...\n","       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","       | Downloading package moses_sample to /root/nltk_data...\n","       |   Unzipping models/moses_sample.zip.\n","       | Downloading package movie_reviews to /root/nltk_data...\n","       |   Unzipping corpora/movie_reviews.zip.\n","       | Downloading package mte_teip5 to /root/nltk_data...\n","       |   Unzipping corpora/mte_teip5.zip.\n","       | Downloading package mwa_ppdb to /root/nltk_data...\n","       |   Unzipping misc/mwa_ppdb.zip.\n","       | Downloading package names to /root/nltk_data...\n","       |   Unzipping corpora/names.zip.\n","       | Downloading package nombank.1.0 to /root/nltk_data...\n","       | Downloading package nonbreaking_prefixes to\n","       |     /root/nltk_data...\n","       |   Unzipping corpora/nonbreaking_prefixes.zip.\n","       | Downloading package nps_chat to /root/nltk_data...\n","       |   Unzipping corpora/nps_chat.zip.\n","       | Downloading package omw to /root/nltk_data...\n","       | Downloading package omw-1.4 to /root/nltk_data...\n","       | Downloading package opinion_lexicon to /root/nltk_data...\n","       |   Unzipping corpora/opinion_lexicon.zip.\n","       | Downloading package panlex_swadesh to /root/nltk_data...\n","       | Downloading package paradigms to /root/nltk_data...\n","       |   Unzipping corpora/paradigms.zip.\n","       | Downloading package pe08 to /root/nltk_data...\n","       |   Unzipping corpora/pe08.zip.\n","       | Downloading package perluniprops to /root/nltk_data...\n","       |   Unzipping misc/perluniprops.zip.\n","       | Downloading package pil to /root/nltk_data...\n","       |   Unzipping corpora/pil.zip.\n","       | Downloading package pl196x to /root/nltk_data...\n","       |   Unzipping corpora/pl196x.zip.\n","       | Downloading package porter_test to /root/nltk_data...\n","       |   Unzipping stemmers/porter_test.zip.\n","       | Downloading package ppattach to /root/nltk_data...\n","       |   Unzipping corpora/ppattach.zip.\n","       | Downloading package problem_reports to /root/nltk_data...\n","       |   Unzipping corpora/problem_reports.zip.\n","       | Downloading package product_reviews_1 to /root/nltk_data...\n","       |   Unzipping corpora/product_reviews_1.zip.\n","       | Downloading package product_reviews_2 to /root/nltk_data...\n","       |   Unzipping corpora/product_reviews_2.zip.\n","       | Downloading package propbank to /root/nltk_data...\n","       | Downloading package pros_cons to /root/nltk_data...\n","       |   Unzipping corpora/pros_cons.zip.\n","       | Downloading package ptb to /root/nltk_data...\n","       |   Unzipping corpora/ptb.zip.\n","       | Downloading package punkt to /root/nltk_data...\n","       |   Unzipping tokenizers/punkt.zip.\n","       | Downloading package qc to /root/nltk_data...\n","       |   Unzipping corpora/qc.zip.\n","       | Downloading package reuters to /root/nltk_data...\n","       | Downloading package rslp to /root/nltk_data...\n","       |   Unzipping stemmers/rslp.zip.\n","       | Downloading package rte to /root/nltk_data...\n","       |   Unzipping corpora/rte.zip.\n","       | Downloading package sample_grammars to /root/nltk_data...\n","       |   Unzipping grammars/sample_grammars.zip.\n","       | Downloading package semcor to /root/nltk_data...\n","       | Downloading package senseval to /root/nltk_data...\n","       |   Unzipping corpora/senseval.zip.\n","       | Downloading package sentence_polarity to /root/nltk_data...\n","       |   Unzipping corpora/sentence_polarity.zip.\n","       | Downloading package sentiwordnet to /root/nltk_data...\n","       |   Unzipping corpora/sentiwordnet.zip.\n","       | Downloading package shakespeare to /root/nltk_data...\n","       |   Unzipping corpora/shakespeare.zip.\n","       | Downloading package sinica_treebank to /root/nltk_data...\n","       |   Unzipping corpora/sinica_treebank.zip.\n","       | Downloading package smultron to /root/nltk_data...\n","       |   Unzipping corpora/smultron.zip.\n","       | Downloading package snowball_data to /root/nltk_data...\n","       | Downloading package spanish_grammars to /root/nltk_data...\n","       |   Unzipping grammars/spanish_grammars.zip.\n","       | Downloading package state_union to /root/nltk_data...\n","       |   Unzipping corpora/state_union.zip.\n","       | Downloading package stopwords to /root/nltk_data...\n","       |   Unzipping corpora/stopwords.zip.\n","       | Downloading package subjectivity to /root/nltk_data...\n","       |   Unzipping corpora/subjectivity.zip.\n","       | Downloading package swadesh to /root/nltk_data...\n","       |   Unzipping corpora/swadesh.zip.\n","       | Downloading package switchboard to /root/nltk_data...\n","       |   Unzipping corpora/switchboard.zip.\n","       | Downloading package tagsets to /root/nltk_data...\n","       |   Unzipping help/tagsets.zip.\n","       | Downloading package timit to /root/nltk_data...\n","       |   Unzipping corpora/timit.zip.\n","       | Downloading package toolbox to /root/nltk_data...\n","       |   Unzipping corpora/toolbox.zip.\n","       | Downloading package treebank to /root/nltk_data...\n","       |   Unzipping corpora/treebank.zip.\n","       | Downloading package twitter_samples to /root/nltk_data...\n","       |   Unzipping corpora/twitter_samples.zip.\n","       | Downloading package udhr to /root/nltk_data...\n","       |   Unzipping corpora/udhr.zip.\n","       | Downloading package udhr2 to /root/nltk_data...\n","       |   Unzipping corpora/udhr2.zip.\n","       | Downloading package unicode_samples to /root/nltk_data...\n","       |   Unzipping corpora/unicode_samples.zip.\n","       | Downloading package universal_tagset to /root/nltk_data...\n","       |   Unzipping taggers/universal_tagset.zip.\n","       | Downloading package universal_treebanks_v20 to\n","       |     /root/nltk_data...\n","       | Downloading package vader_lexicon to /root/nltk_data...\n","       | Downloading package verbnet to /root/nltk_data...\n","       |   Unzipping corpora/verbnet.zip.\n","       | Downloading package verbnet3 to /root/nltk_data...\n","       |   Unzipping corpora/verbnet3.zip.\n","       | Downloading package webtext to /root/nltk_data...\n","       |   Unzipping corpora/webtext.zip.\n","       | Downloading package wmt15_eval to /root/nltk_data...\n","       |   Unzipping models/wmt15_eval.zip.\n","       | Downloading package word2vec_sample to /root/nltk_data...\n","       |   Unzipping models/word2vec_sample.zip.\n","       | Downloading package wordnet to /root/nltk_data...\n","       | Downloading package wordnet2021 to /root/nltk_data...\n","       | Downloading package wordnet2022 to /root/nltk_data...\n","       |   Unzipping corpora/wordnet2022.zip.\n","       | Downloading package wordnet31 to /root/nltk_data...\n","       | Downloading package wordnet_ic to /root/nltk_data...\n","       |   Unzipping corpora/wordnet_ic.zip.\n","       | Downloading package words to /root/nltk_data...\n","       |   Unzipping corpora/words.zip.\n","       | Downloading package ycoe to /root/nltk_data...\n","       |   Unzipping corpora/ycoe.zip.\n","       | \n","     Done downloading collection all\n"]},{"name":"stdout","output_type":"stream","text":["\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> q\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["from nltk.corpus import stopwords # Import the stop word list\n","print(stopwords.words(\"english\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xLtblyTP159W","executionInfo":{"status":"ok","timestamp":1709631083211,"user_tz":-540,"elapsed":268,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"924ebc8d-2d09-4517-8f90-62a239f55fac"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}]},{"cell_type":"code","source":["# Remove stop words from \"words\"\n","words = [w for w in words if not w in stopwords.words(\"english\")]\n","print(words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gc2KIb5P16Ao","executionInfo":{"status":"ok","timestamp":1709631085275,"user_tz":-540,"elapsed":265,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"39582507-03a0-483d-df4b-bcc3f488a576"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter']\n"]}]},{"cell_type":"code","source":["def review_to_words( raw_review ):\n","    # Function to convert a raw review to a string of words\n","    # The input is a single string (a raw movie review), and\n","    # the output is a single string (a preprocessed movie review)\n","    #\n","    # 1. Remove HTML\n","    review_text = BeautifulSoup(raw_review).get_text()\n","    #\n","    # 2. Remove non-letters\n","    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n","    #\n","    # 3. Convert to lower case, split into individual words\n","    words = letters_only.lower().split()\n","    #\n","    # 4. In Python, searching a set is much faster than searching\n","    #   a list, so convert the stop words to a set\n","    stops = set(stopwords.words(\"english\"))\n","    #\n","    # 5. Remove stop words\n","    meaningful_words = [w for w in words if not w in stops]\n","    #\n","    # 6. Join the words back into one string separated by space,\n","    # and return the result.\n","    return( \" \".join( meaningful_words ))"],"metadata":{"id":"ABb7bBwg16D3","executionInfo":{"status":"ok","timestamp":1709631089078,"user_tz":-540,"elapsed":270,"user":{"displayName":"현","userId":"10086973512213957086"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["clean_review = review_to_words( train[\"review\"][0] )\n","print (clean_review)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olMeLiu816HN","executionInfo":{"status":"ok","timestamp":1709631098473,"user_tz":-540,"elapsed":380,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"1f2687b4-668b-46a3-907c-a07d3ce4c682"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter\n"]}]},{"cell_type":"code","source":["# Get the number of reviews based on the dataframe column size\n","num_reviews = train[\"review\"].size\n","\n","# Initialize an empty list to hold the clean reviews\n","clean_train_reviews = []\n","\n","# Loop over each review; create an index i that goes from 0 to the length\n","# of the movie review list\n","for i in range( 0, num_reviews ):\n","    # Call our function for each one, and add the result to the list of\n","    # clean reviews\n","    clean_train_reviews.append( review_to_words( train[\"review\"][i] ) )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJvgGW3c2NMD","executionInfo":{"status":"ok","timestamp":1709631173843,"user_tz":-540,"elapsed":12109,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"348b7d43-7625-4cca-e830-ab935bdfe8e4"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-23-4ca808e18148>:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  review_text = BeautifulSoup(raw_review).get_text()\n"]}]},{"cell_type":"code","source":["print(\"Cleaning and parsing the training set movie reviews...\\n\")\n","clean_train_reviews = []\n","num_reviews = len(train)\n","\n","for i in range(0, num_reviews):\n","    # If the index is evenly divisible by 1000, print a message\n","    if (i + 1) % 1000 == 0:\n","        print(\"Review %d of %d\\n\" % (i + 1, num_reviews))\n","    clean_train_reviews.append(review_to_words(train[\"review\"][i]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hPBoUwv2NPS","executionInfo":{"status":"ok","timestamp":1709631188462,"user_tz":-540,"elapsed":11672,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"db1cf7ce-3195-41de-f300-f4293ee6531d"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaning and parsing the training set movie reviews...\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-23-4ca808e18148>:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  review_text = BeautifulSoup(raw_review).get_text()\n"]},{"output_type":"stream","name":"stdout","text":["Review 1000 of 25000\n","\n","Review 2000 of 25000\n","\n","Review 3000 of 25000\n","\n","Review 4000 of 25000\n","\n","Review 5000 of 25000\n","\n","Review 6000 of 25000\n","\n","Review 7000 of 25000\n","\n","Review 8000 of 25000\n","\n","Review 9000 of 25000\n","\n","Review 10000 of 25000\n","\n","Review 11000 of 25000\n","\n","Review 12000 of 25000\n","\n","Review 13000 of 25000\n","\n","Review 14000 of 25000\n","\n","Review 15000 of 25000\n","\n","Review 16000 of 25000\n","\n","Review 17000 of 25000\n","\n","Review 18000 of 25000\n","\n","Review 19000 of 25000\n","\n","Review 20000 of 25000\n","\n","Review 21000 of 25000\n","\n","Review 22000 of 25000\n","\n","Review 23000 of 25000\n","\n","Review 24000 of 25000\n","\n","Review 25000 of 25000\n","\n"]}]},{"cell_type":"code","source":["print(\"Creating the bag of words...\\n\")\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n","# bag of words tool.\n","vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features=5000)\n","\n","# fit_transform() does two functions: First, it fits the model\n","# and learns the vocabulary; second, it transforms our training data\n","# into feature vectors. The input to fit_transform should be an iterable\n","# containing text data.\n","train_data_features = vectorizer.fit_transform(clean_train_reviews)\n","\n","# Numpy arrays are easy to work with, so convert the result to an\n","# array\n","train_data_features = train_data_features.toarray()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MLGSoZ792NSP","executionInfo":{"status":"ok","timestamp":1709631193787,"user_tz":-540,"elapsed":2755,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"cd9e1867-10d7-487c-ea96-0fa61cedc078"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating the bag of words...\n","\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# 예시로 임의의 문장들을 만들어서 학습시키기 위한 데이터셋을 생성합니다.\n","texts = [\"This is an example sentence.\",\n","         \"Another example sentence.\",\n","         \"Yet another example sentence.\"]\n","\n","# TfidfVectorizer를 사용하여 단어 벡터를 생성합니다.\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(texts)\n","\n","# 단어들의 목록을 가져옵니다.\n","vocab = vectorizer.get_feature_names_out()\n","\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sWFr3rRu2V-S","executionInfo":{"status":"ok","timestamp":1709631340974,"user_tz":-540,"elapsed":6,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"70707140-c609-4527-a140-a961d2a3eb99"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["['an' 'another' 'example' 'is' 'sentence' 'this' 'yet']\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Sum up the counts of each vocabulary word\n","dist = np.sum(train_data_features, axis=0)\n","\n","# For each, print the vocabulary word and the number of times it\n","# appears in the training set\n","for tag, count in zip(vocab, dist):\n","    print (count, tag)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cCRLuNB_2WBW","executionInfo":{"status":"ok","timestamp":1709631354778,"user_tz":-540,"elapsed":705,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"c5cf09b0-3ebb-47c1-82dc-98805a2040bc"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["187 an\n","125 another\n","108 example\n","454 is\n","1259 sentence\n","85 this\n","116 yet\n"]}]},{"cell_type":"code","source":["print (\"Training the random forest...\")\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Initialize a Random Forest classifier with 100 trees\n","forest = RandomForestClassifier(n_estimators = 100)\n","\n","# Fit the forest to the training set, using the bag of words as\n","# features and the sentiment labels as the response variable\n","#\n","# This may take a few minutes to run\n","forest = forest.fit( train_data_features, train[\"sentiment\"] )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"javwk8SI2WEW","executionInfo":{"status":"ok","timestamp":1709631409719,"user_tz":-540,"elapsed":35812,"user":{"displayName":"현","userId":"10086973512213957086"}},"outputId":"0d69e821-a6c5-4c71-b438-e2567c0908f8"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Training the random forest...\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Read the test data\n","test = pd.read_csv(\"/content/drive/MyDrive/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n","\n","# Initialize TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","\n","# Transform the training data to feature vectors\n","train_data_features = vectorizer.fit_transform(train[\"review\"])\n","\n","# Initialize Random Forest Classifier\n","forest = RandomForestClassifier(n_estimators=100)\n","\n","# Fit the forest to the training set, using the bag of words as features and the sentiment labels as the response variable\n","forest = forest.fit(train_data_features, train[\"sentiment\"])\n","\n","# Verify that there are 25,000 rows and 2 columns\n","print(test.shape)\n","\n","# Create an empty list and append the clean reviews one by one\n","num_reviews = len(test[\"review\"])\n","clean_test_reviews = []\n","\n","print(\"Cleaning and parsing the test set movie reviews...\\n\")\n","for i in range(0, num_reviews):\n","    if (i + 1) % 1000 == 0:\n","        print(\"Review %d of %d\\n\" % (i + 1, num_reviews))\n","    clean_review = review_to_words(test[\"review\"][i])\n","    clean_test_reviews.append(clean_review)\n","\n","# Get a bag of words for the test set, and convert to a numpy array\n","test_data_features = vectorizer.transform(clean_test_reviews)\n","test_data_features = test_data_features.toarray()\n","\n","# Use the random forest to make sentiment label predictions\n","result = forest.predict(test_data_features)\n","\n","# Copy the results to a pandas dataframe with an \"id\" column and a \"sentiment\" column\n","output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": result})\n","\n","# Use pandas to write the comma-separated output file\n","output.to_csv(\"Bag_of_Words_model.csv\", index=False, quoting=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aCUXaIGK2WHd","outputId":"91ee4f3f-18cf-450c-862d-a2ee4d6dd1b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(25000, 2)\n","Cleaning and parsing the test set movie reviews...\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-23-4ca808e18148>:7: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  review_text = BeautifulSoup(raw_review).get_text()\n"]},{"output_type":"stream","name":"stdout","text":["Review 1000 of 25000\n","\n","Review 2000 of 25000\n","\n","Review 3000 of 25000\n","\n","Review 4000 of 25000\n","\n","Review 5000 of 25000\n","\n","Review 6000 of 25000\n","\n","Review 7000 of 25000\n","\n","Review 8000 of 25000\n","\n","Review 9000 of 25000\n","\n","Review 10000 of 25000\n","\n","Review 11000 of 25000\n","\n","Review 12000 of 25000\n","\n","Review 13000 of 25000\n","\n","Review 14000 of 25000\n","\n","Review 15000 of 25000\n","\n","Review 16000 of 25000\n","\n","Review 17000 of 25000\n","\n","Review 18000 of 25000\n","\n","Review 19000 of 25000\n","\n","Review 20000 of 25000\n","\n","Review 21000 of 25000\n","\n","Review 22000 of 25000\n","\n","Review 23000 of 25000\n","\n","Review 24000 of 25000\n","\n","Review 25000 of 25000\n","\n"]}]}]}